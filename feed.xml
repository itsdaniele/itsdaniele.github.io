<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://danielep.xyz/feed.xml" rel="self" type="application/atom+xml"/><link href="https://danielep.xyz/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-18T13:04:44+00:00</updated><id>https://danielep.xyz/feed.xml</id><title type="html">Daniele Paliotta</title><subtitle></subtitle><entry><title type="html">On the AI Sentience Debate</title><link href="https://danielep.xyz/blog/2022/sentience/" rel="alternate" type="text/html" title="On the AI Sentience Debate"/><published>2022-11-07T15:12:00+00:00</published><updated>2022-11-07T15:12:00+00:00</updated><id>https://danielep.xyz/blog/2022/sentience</id><content type="html" xml:base="https://danielep.xyz/blog/2022/sentience/"><![CDATA[<p>This article is available on my <a href="https://danielepaliotta.substack.com/p/on-the-ai-sentience-debate">Substack</a>.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Some thoughts]]></summary></entry><entry><title type="html">Graphs at ICLR 2021</title><link href="https://danielep.xyz/blog/2021/graphs-iclr2021/" rel="alternate" type="text/html" title="Graphs at ICLR 2021"/><published>2021-06-06T15:12:00+00:00</published><updated>2021-06-06T15:12:00+00:00</updated><id>https://danielep.xyz/blog/2021/graphs-iclr2021</id><content type="html" xml:base="https://danielep.xyz/blog/2021/graphs-iclr2021/"><![CDATA[<p>The field of geometric deep learning is booming, there’s no way around it. And graph neural networks are the rockstars, sitting in the driver seat.</p> <p>Graphs were clearly at the center of a lot of attention at ICLR 2021. Not only thanks to <a href="https://iclr.cc/virtual/2021/invited-talk/3717">Michael Bronstein’s amazing keynote speech on geometric deep learning</a>, but also because of a number of interesting papers ranging from theory, methods and applications.</p> <p>In general, I feel like there we are still in the latent face of a revolution in the field of geometric deep learning, although I still have no idea about the possible magnitute of such revolution. Will message passing be superceded by something else? Will there be some great unification of deep learning architectures? It sure looks exciting.</p> <p>As for ICLR 2021, I feel like the works were interesting, but mostly incremental, and there was no big breakthrough.</p> <p>In this post, I will try to summarize what some of these papers were mainly about, what seem to be the current trends, and what we can expect from the future.</p> <hr/> <h1 id="wasserstein-embedding-for-graph-learning">Wasserstein Embedding For Graph Learning</h1> <p>This paper is all about the development of a new framework for embedding entire graphs. Graph embedding methods typically require several steps, namely i) feature aggregation to produce node embeddings, followed by some type of ii) graph coarsening or pooling, and a final iii) classification step.</p> <p>However, current methods for graph classification fail to scale with the number of graphs in the dataset. As an example, kernel methods typically need to compute pairwise similarities between the graphs, which is computationally heavy when the dataset is large.</p> <p>The approach proposed in the paper employs optimal transport methods to measure the dissimilarity between the graphs. Similar approaches already exist, but the authors here derive a direct linar map to compute the embeddings, making the algorithm linear in the number of input graphs.</p> <p>The first step in the process is to produce the node embeddings, which is done via a simple non-parametric diffusion process. After that, the node representations across layers are concatenated to get the final vectors. These representations are then mapped to the Wasserstein embeddings, with the idea that the L2 distance between the resulting vectors approximates the 2-Wasserstein distance between them.</p> <p>These final representation can then be used in any kind of downstream classifier or kernel method.</p> <p>With this approach, the authors achieve SOTA or competitive results on the <em>obgb-molhiv</em> dataset or molecular property prediction and on several TUD benchmark datasets.</p> <hr/> <h1 id="simple-spectral-graph-convolution">Simple Spectral Graph Convolution</h1> <p>Another graph convolution method to combat oversmoothing, derived from spectral principles.</p> <p>Here, the authors propose a spectral-based graph convolutional layer, called Simple Spectral Graph Convolution (<strong>S2GC</strong>), which is based on the <strong>Markov Diffusion Kernel</strong> (MDK). The authors show that S2GC is capable of aggregating k-hop neighbourhood information without oversmoothing.</p> <p>The initial proposed layer has the form <img src="/assets/img/ssgc1.png" width="70%"/></p> <p>with \(\hat{T}\) being the normalized adjacency matrix with added self-loops.</p> <p>However, this formulation can still incur in oversmoothing. Thus, the authors incorporate the possibility to interpolate between self-information and neighbor aggregation:</p> <p><img src="/assets/img/ssgc2.png" width="70%"/></p> <p>The model is evaluated on the tasks of text and node classification, as well as node clustering and community detection. In general, the results are competitive with previous models, but this new formulation appears to be able to combat oversmoothing as the receptive field increases.</p> <p>It is also showed in the paper that the proposed filter, by design, will give the highest weight to the closest neighborhood of a node, and that the model can incorporate larger receptive fields without undermining contributions of smaller receptive fields, which might be the reason why this model doesn’t suffer from oversmoothing.</p> <hr/> <h1 id="adaptive-universal-generalized-pagerank-graph-neural-network">Adaptive Universal Generalized PageRank Graph Neural Network</h1> <p>The goal here is to build a model that can adapt to both homophily and heterophily settings, and combat oversmoothing. The proposed approach incorporates the generalized page rank (GPR) algorithm in Graph Neural Networks (GNNs).</p> <p>Simply put, the GPR algorithm assigns scores to nodes in a graph that are then used for clustering purposes.</p> <p>The GPR + GNN process is as follows:</p> <p><img src="/assets/img/pagerank.png" width="70%"/></p> <p>Here, the initial node representation is derived by a neural network \(f_{\theta}\). After that, the typical graph diffusion is performed using the symmetric adjacency matrix in order to get representations that aggregate information from 1 to \(k\)-hop neighborhoods. To get the final representations, the hidden representaions from 1 to \(k\) are summed and weighted by values \(\gamma_k\). These weights are trained jointly with \(f_{\theta}\).</p> <p>As the authors point out, the weights \(\gamma_k\) give the model the ability to adaptively control the contribution of each propagation step and adjust it to the node label pattern, thus adapting to both homophily and etherophily settings.</p> <p>Moreover, oversmoothing should be combated by the model by assigning less weight to large-range propagation steps whenever they are not beneficial in the training procedure.</p> <hr/> <h1 id="how-to-find-your-friendly-neighborhood-graph-attention-design-with-self-supervision">How To Find Your Friendly Neighborhood: Graph Attention Design With Self-Supervision</h1> <p>The paper is an exploration of attention in graph neural networks. Moreover, the authors propose to use attention-based, self-supervised link-prediction as an auxiliary task when doing node classification.</p> <p>Apparently, there is room to improve self-attention mechanism in graph neural networks as, for example, GATs have typically showed performance improvements but the improvements are not consistent across datasets, and it’s not even clear what graph attention actually learns.</p> <p>Let’s recall that the GAT’s attention comes in the form of \(a^T[Wh_i\mid\mid Wh_j]\). In addition to this, the authors investigate dot-product attention, which is in the form \((Wh_i) * (Wh_j)\).</p> <p>In the proposed self-supervision framework, the attention is used to predict the presence/absence of edges between node pairs. In essence, it’s an auxiliary link prediction task. The authors explore four different attention mechanisms: the original one from GAT, dot product attention, scaled dot product, and a mix of dot product and GAT attention.</p> <p>The final loss is the combination of the cross-entropy on the node labels (for the node classification task), and the self-supervised graph attention losses, plus an L2 penalty term.</p> <p>Given this framework, the authors pose some research question, which I’ll summarize here.</p> <ol> <li><strong>Does graph attention learn label agreement?</strong> The authors here seem to think that, due to oversmoothing in GAT, if an edge exists between nodes with different labels, that it will be hard to distinguish them. Thus, they postulate that ideal attention should give all the weights to label-agreed neighbors. In their experiments, they show that GAT attention learns label-agreement better that dot-product.</li> <li><strong>Is graph attention predictive for edge presence?</strong> On the link predictiont ask, dot-product attention consistently outperforms GAT attention.</li> <li><strong>Which graph attention should we use for given graphs?</strong> The hypothesis here is that different attention mechanisms have will have different abilities to model graphs under various homophily and average degree. Here, the best performing model in low-homphily settings employs scaled dot-product attention with self-supervision, showing that self-supervision can be useful. However, when homphily and average degree are high enough, there is no difference in performance between all the models, including a vanilla GCN.</li> </ol> <p>All of the experiments done so far were done using synthetic dataset, as they allow for controlling several graph properties. However, the authors show that the design choice generalize to many real-world datasets.</p> <hr/> <h1 id="learning-mesh-based-simulations-with-graph-networks">Learning Mesh-Based Simulations With Graph Networks</h1> <p>Here the authors introduce <strong>MeshGraphNets</strong>, a framework for learning mesh-based simulations using graph neural networks. These simulations allow modelling of complex physical systems, such as cloth, fluid dynamics, and air flow.</p> <p>The original simulation meshes are represented by a set of mesh nodes and edges. Each node is associated with a coordinate in mesh space, and with the value of the quantity that the system is modelling. Some systems, called <em>Lagrangian</em>, are also endowed with a 3D so-called <em>world-space</em> coordinate for the nodes.</p> <p>The proposed model first translates the mesh represantion into a multigraph, with the Lagrangian systems having additional”word”edges to enable interactions that might be local in 3D space but non-local in mesh space. These edges are created by spatial proximity.</p> <p>In the graph, positional features are encoded in the edges. The initial features for both nodes and edges are then encoeed with simple MPLs, and that passed though several layers of message-passing.</p> <p><img src="/assets/img/mesh.png" width="90%"/></p> <p>The latent node features for the nodes are finally translated by a decoder MLP into output features \(p_i\). These features are interpreted as derivatives of the system quantitiy that we are modelling, so that they can be used to update the state of the system at the next timestep.</p> <p>The resulting simulations, which can be seen <a href="https://sites.google.com/view/meshgraphnets">here</a>, tend o run 1-2 orders of magnitude faster that the simulations they were trained on.</p> <hr/> <h1 id="learning-parametrized-graph-shift-operators">Learning Parametrized Graph Shift Operators</h1> <p>This paper provides and analysis for graph shift operators (GSO)* in graph learning, as well as a strategy for learning parametrized shift operators on graphs.</p> <p>The parametrization is achieved in the following way:</p> <p><img src="/assets/img/gso1.png" width="70%"/></p> <p>where \(D_a\) is the degre matrix, and \(A_a\) is the adjacency matrix with self-loops.</p> <p>Depending on the parameters values, one can retrieve commonly used graph shift operators, as shown in the following table:</p> <p><img src="/assets/img/gso2.png" width="70%"/></p> <p>The authors then show how to include this new parametrized GSO in common GNN architectures.</p> <p>The exposition continues with a brief theoretical analysis, where for example it is shown that the parametrized GSO has real eigenvalues and eigenvectors, making it feasible to use in exhisting spectral network analysis frameworks where this property is required. Some other bounds useful for numerical stability are derived in the paper.</p> <hr/> <h1 id="on-the-bottleneck-of-graph-neural-networks-and-its-practical-implications">On the Bottleneck of Graph Neural Networks and its Practical Implications</h1> <p>One of the main issues with graphnets is the <em>bottleneck</em> issue: GNNs struggle to propagate information between distant nodes in the graph. This paper relates this failure mode to the <em>over-squashing</em> problem, which happens when the network is not able to compress exponentially-growing information into fixed-sized vectors.</p> <p>As a result, traditional GNNs perform poorly when the prediction task depends on long-range interaction. Moreover, it is shown that GNNs that absorm incoming edges equally, such as <em>GCN</em> and <em>GIN</em>, are more subsceptible to this issue than <em>GAT</em> of <em>GGNN</em>.</p> <p>In most literature, GNNs were observed not to benefit from more than a few layers. The common explaantion for this is <em>oversmoothing</em>: node representations become indistinguishable when the number of layers increases. For long-range task, however, the paper hypothesizes that the explanation for the limited perfomance lies in <em>oversquashing</em>.</p> <p>In fact, the bottleneck issue here is very similar to the issue for seq2seq models, which typically suffer from a bottleneck problem in the decoder when attention is not used, since the receptive field of a ndoe grows linearly with the number of steps.</p> <p>For graph, the issue is strictly worse, since the number of nodes that fall in the receptive field of a target node grows exponentially with the number of message-passing steps.</p> <p>The <em>problem radius</em> here is defined as a useful measure for defining the problem, and it correponds to the problem’s required range of interaction. Clearly, this is typically unknown in advance, and is usually approximated empirially by tuning the number of layers.</p> <p>When a prediciton problem as a problem radius \(r\), the GNN muyst have as many layers \(K\) as \(r\). Howeverly, since the number of nodes in the receptive field grows exponentially with \(K\), the network need to squash an exponentially-growing amount of intomation into a fixed-size vector, and so crucial messages might fail to reach thei distant destinations, and the model would learn only short-ranged signals fromt he training data and fail to generalize at test time.</p> <p>The authors show the <strong>NEIGHBORMATCH</strong> task a toy task that exhibits the need for long-range interactions between nodes.</p> <p>Examples of tasks that require long-range interaction appear in the prediciton fo chemical properties of molecules, which might depend on the combination of atoms that reside on opposite sides of the molecule.</p> <p>On the toy dataset, it is shown that, even if enogh layers are built into the network, the models underfit when the number of layers increases. The GAT and GGNN fails later that GCN and GIN. this difference can be exaplined by the neighbor aggregation computation. GCN and GIN aggregate all neighbors before combining them with the target node’s representation. Thus they must compress the information flowing from all the leaves into a single vector, and only afterwars interact with the target node’s own representation. In contrast, GAT uses attention to weight incoming messages given tha target node’s representation. At the last layer only, the target node can ignore the irrelevant incoming edge, and absorb only the relevant edge.</p> <p>A question posed is: if all GNNs have reached low training accuracy, how do these GNN models usually fit the training data in public datasets of long-range problems? The hypothesis is that they overfiit short-range signals and artifacts from the training set, rather that learning the long-range information that was sqashed in the bottleneck. Thus, they generalize poorly at test time.</p> <p>Simple solution: Adding a fully-adjacent (FA) layer in the last layer. the FA later has every pair of nodes connected by an edge. This allows the topology-aware representations (that arrived at the last layer) to interact directly and consider nodes beyond thei original neighbors. This significanly reduces the error rate.</p> <p>Moreover, the authors try to assess whether the issue might have to do with under-reaching rather that oversquashing. However, that is not the case, as they show that the networks already had enough layer to reach the radius needes for they task.</p> <hr/> <h1 id="expressive-power-of-invariant-and-equivariant-graph-neural-networks">Expressive Power of Invariant and Equivariant Graph Neural Networks</h1> <p>This theoretical paper compares the expressive power of three types of invariant and equivariant GNNs against the Weisfeiler-Lehman (WL) tests, proves function approximation results for these GNNs, and demonstrates that 2-FGNN_E works well for the quadratic assignment problem. One of the main results is showing that k-FGNN are as powerful as the (k-1)-Weisfeiler-Lehman test.</p> <hr/> <h1 id="and-so">And so?</h1> <p>So, what can we expect from the immediate future of geometric deep learning? One thing I would bet on is that we’ll see a lot of work on self-supervision and pre-training of larger architectures. As we know, increasizing the size of most graph models does not necessarily scales perfomance. It’s not a transformer world, and attention is not all we need. I think we are starting to understand why this is, and the increasily large body of theoretical work might help us in building model that seamlessly scale to huge graph without big degradations in performance.</p> <p>Also, what about message passing? Will we move pass the bottleneck? There is some reason to think that we might be able to extract connectivity-aware features, and that would be enough to discard the graph structure later on and training without any big bottleneck.</p> <p>And what about attention? It has won the computer vision and NLP context (altough now it’s already so old-fashioned. Yes I am talking about MLPs), but with graphs it’s not so clear yet.</p> <p>I know, it’s more questions than answers at this point. But if I had the answers I’d probably write a paper about it.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Geometric Deep Learning at ICLR 2021]]></summary></entry><entry><title type="html">Teaching Myself Quantum Mechanics, Part one.</title><link href="https://danielep.xyz/blog/2021/qm-1/" rel="alternate" type="text/html" title="Teaching Myself Quantum Mechanics, Part one."/><published>2021-02-04T15:12:00+00:00</published><updated>2021-02-04T15:12:00+00:00</updated><id>https://danielep.xyz/blog/2021/qm-1</id><content type="html" xml:base="https://danielep.xyz/blog/2021/qm-1/"><![CDATA[<p>This series is about me learning something in my spare time. More specifically, it’s about me learning quantum mechanics, as a way of getting back into physics, and as a way of making good science.</p> <p>I’ll explain: I have recently started a PhD in machine learning at the University of Geneva. The main project I’ll be working on is about applying machine learning and statistical methods to physics, mainly high energy physics (HEP) and solar astronomy. I am part of a very strong multidisciplinary team of physicists and AI people, so I could probably do without a strong physics background. Still, I feel like that can’t hurt, right?</p> <p>So there you have it: I want to study and understand physics in order to do better research.</p> <p>But that’s not the whole story. I also love physics, and I have tortured myself over and over as to whether I should have majored in physics rather that CS during my university career. This is not to say that I am any good at it. I only had one big physics exam during my bachelor (topics were kinematics, some classical mechanics, electromagnetism), and all the rest of my knowledge is made of bits and pieces coming from random youtube lectures and non-technical books I have read over the years. So yeah, it’s finally time to get serious.</p> <h3 id="why-start-with-quantum-mechanics">Why start with Quantum Mechanics?</h3> <p>Because I find it exciting, and that’s probably enough. Starting with classical mechanics would likely bore me to death. I know QM is not easy or intuitive, and that math can get hard. However, I don’t feel completely ill-equipped. Thanks to my engineering and ML background, I know one thing or two about probabilities, linear algebra, Fourier series and complex numbers, which I have been told are a substantial part of the required background.</p> <h3 id="what-is-your-study-method">What is your study method?</h3> <p>I usually try to be extremely deliberate about my study method. I won’t go into the details here, but I use active recall and spatial repetition tools such as Anki so that remembering what I study is no longer a random event but a purposeful act. There are many interesting resources about such techniques, such as <a href="http://augmentingcognition.com/ltm.html">this</a>, and <a href="https://aliabdaal.com/spacedrepetition/">this</a>. Michael Nielsen has even developed an <a href="https://quantum.country/qcvc">introduction to quantum computing</a> delivered in a “new mnemonic medium which makes it almost effortless to remember what you read” (hint: it’s active recall and spaced repetition).</p> <p>As for the learning resources, I have chosen two introductory books: one is from the theoretical minimum series from the great Leonard Susskind. This is supposed to be a very simple introduction, starting from first principles. I have already flipped through the pages, and there actually seems to be quite a lot of content, or at least more than you would expect from a book of this kind.</p> <p>The second book it more technical, and it comes up over and over as a suggestion for learning QM: Griffiths’ Introduction to Quantum Mechanics. Honestly, here I have no idea what to expect.</p> <p>So here’t the plan: I’ll go through both books simultaneously and try to solve most exercises. I’ll also create flashcards and diagrams of the concepts, and let Anki do the rest. Additionally, I’ll write this blog series, which hopefully helps me understand things better. So if you are here to learn, don’t just count on my writing.</p> <h3 id="requirements">Requirements</h3> <p>The requirements are basic linear algebra and basic probability. And complex numbers. Some notions of classical physics can also help.</p> <hr/> <h2 id="spins-and-qubits">Spins and Qubits</h2> <p>The fact that we cannot fully grasp some things about the inner workings of our universe should come at no surprise. After all, we evolved to survive, not to understand. Ok yes, we have a strong intuition for many classical-mechanical things, but the reason is that things that behave classically are closer to our experience than things that behave relativistically or in a quantum mechanical way. So what we can do here is simple: let’s just go with the math and with the experiment. I don’t expect any of the concepts I encounter to be “relatable”. I am going to trust the result of the experiments, and link that to the math which hopefully explains them. And I’ll start with the <strong>spin</strong>.</p> <p>Sooo, the spin is an extra degree of freedom that some particle have (all of them? I don’t know).</p> <p>This spin is a very quantum mechanical concept, which is why trying to visualize it classically would miss the point.</p> <p>Now, the spin of a particle is a quantum systems on its own right. In fact, it’s the “simplest and the most quantum of systems”, according to Susskind. Just by going through a very simple experiment involving the spin, we can already start to notice some important differences between classical physics and quantum mechanics.</p> <p>The experiment involves a measuring system, which we call \(\mathcal{A}\), that records the state of the spin of our system (we’ll call it \(\sigma\)). We can orient the measuring apparatus in order to measure this “something” along a specified axis. As a test, we rotate the apparatus along the \(z\)-axis, and notice that our measurement has the value of \(1\) or \(-1\).</p> <p>Now, if we reset the apparatus to measure again the spin along the \(z\)-axis, and assume the simple evolution law</p> \[\sigma(n+1) = \sigma(n)\] <p>which tells us that the state of the system remains unchanged through time, then the previous measurement should be confirmed, again and again. We say that the measurement of a state <strong>prepares the system.</strong></p> <p>In quantum experiments, performing the same measurement repeatedly gives the same answer until the system is prepared differently. We will see this in more detail now.</p> <p>After we have prepared the system in the state \(\sigma_z = 1\), let’s now rotate the apparatus of, say, 90 degrees, and measure the spin along the \(x\)-axis. Something weird happens. Sometimes we get +1, sometimes we get -1. It looks like the result is no longer deterministic, but in some peculiar way. In fact, if we repeat the experiment multiple times, we find that the average value of the measurement along the \(x\)-axis is \(0\).</p> <p>In fact, this is generalizable to any direction. We can pick any direction \(\hat{m}\) and prepare a spin so that the apparatus measures a \(+1\) in that direction. Then, we rotate the apparatus to any other direction \(\hat{n}\). Again, the result of the measurement will still be either \(+1\) or \(-1\), but the expected value will be equal to the projection of \(\hat{m}\) over \(\hat{n}\) , that is, \(\cos{\theta}\), with \(\theta\) being the angle between the vectors.</p> <p>In <em>bra-ket notation</em> (we’ll talk about this more in detail)<em>:</em></p> \[\langle{\sigma}\rangle = \hat{m} \cdot\hat{n} = \cos{\theta}\] <p>Notice that this is the same result that we would expect in classical physics for some vector quantity that we measure. However, in the classical sense, the result would be deterministic. In QM, the result is statistically random, but the average converges to the classical result.</p> <h2 id="vector-spaces-inner-products-bases">Vector spaces, inner products, bases.</h2> <p>In quantum mechanics the space of the states is a vector space. We call vectors <em>ket.</em> This is a <em>ket:</em> \(\vert a\rangle\). Some simple axioms are defined in this space: the sum of two kets is a ket, ket addition is commutative and associative, and some other things like the existence of a 0 vector, an additive inverse, and linearity. Up to this point, everything looks normal, and the vector space we have defined is practically the same as the space of 3-vectors in Euclidean space. However, the space of our <em>kets</em> is a complex vector space, made of complex numbers, and where the multiplication by a scalar value also extends to complex numbers! This is a very abstract concept, but it’s absolutely needed to make the theory work mathematically.</p> <p>Now, if you know something about complex vector, you will likely remember about the complex conjugate. Briefly, given a complex number \(z\), there exists a complex conjugate \(z^*\) that we obtain by reversing the sign of the imaginary part. So if \(z=x+iy = re^{i\theta}\), then \(z^*=x-iy = re^{-i\theta}\). Note that the product of a complex number and its complex conjugate is always positive, yielding \(r^2\) in our case.</p> <p>In the same way, a complex vector space has a <em>dual</em> space that is its complex conjugate vector space. In our case, for every <em>ket</em> \(\vert A\rangle\), we can define a <em>bra</em> vector in that dual space: \(\langle A \vert\), which is essentially its complex conjugate.</p> <p>Now, if you got up to this point, I am pretty sure you remember about dot-products in Euclidean space, right? Well, that dot-product is a specific instance of so-called <em>inner products,</em> which we can also define in our complex space. In out quantum-mechanical magical world, the inner product is defined between a <em>bra</em> and a <em>ket</em>, and the notation is</p> \[\langle B|A \rangle\] <p>and, of course, the result is a complex number. These products are linear, and interchanging bras and kets corresponds to complex conjugation</p> \[\langle B|A \rangle = \langle A|B \rangle ^*\] <p>Concretely, the product is performed in the exact same way as for the dot-product: sum of the products of the components.</p> <p>We can also bring with us some familiar concepts from Euclidean spaces:</p> <ul> <li>A vector is <em>normalized</em> if its inner products with itself is 1.</li> <li>Two vectors are <em>orthogonal</em> if their inner products is 0.</li> </ul> <p>In our vector space, we can also define a <em>basis</em>, which is simply a set of vector that can be used to derive any other vector in the space through linear combinations. It’s often useful, and it is especially in QM, to talk about <em>orthonormal bases</em>, which are <em>bases</em> where the vectors are normalized and orthogonal between each other. Note that the number of vectors in a <em>basis</em> is equivalent to the dimension of the space.</p> <h3 id="quantum-states">Quantum States</h3> <p>With some math tools under our belt, let’s try to formalize the notion of a <em>spin state</em> from earlier. For this task, we will use vectors, and create a representation that captures what we know about how spins behave.</p> <p>Let’s start but labelling all the possible spin states along the three axes. When the apparatus \(\mathcal{A}\) is oriented along the \(z\)-axis, the two possible states are \(\sigma_z = \pm1\). We can call them <em>up</em> and <em>down</em> states and assign them ket vectors \(\vert u\rangle\) and \(\vert d\rangle\). So, when \(\mathcal{A}\) is oriented along the \(z\)-axis and registers \(+1\), the system is in state \(\vert u\rangle\). We also define the states for the directions along the \(x\)-axis and call them \(\vert r\rangle\) and \(\vert l\rangle\) (for <em>right</em> and left), and finally along the \(y\)-axis which we call \(\vert i\rangle\) and \(\vert o\rangle\) (<em>in</em> and <em>out</em>).</p> <p>Now, a consequence of this formalization is that the space of the states for a single spin has only two dimensions. This means that all possible states of a spin can be represented in a two-dimensional vector space. If we choose \(\vert u\rangle\) and \(\vert d\rangle\) as the basis vectors for this space, following by the definition of a <em>basis</em>, we can then write any other state as a linear combination (or <em>superposition) of these two vectors.</em></p> <p>Thus, a generic state \(\vert A\rangle\) comes in the form</p> \[|A\rangle = \alpha_u|u\rangle + a_d|d\rangle\] <p>where \(\alpha_u\) and \(\alpha_d\) are the components of \(\vert A\rangle\) along the two basis vectors.</p> <p>We can retrieve these components through an inner product (which is essentially a projection) of the state vector on the respective basis vectors:</p> \[\alpha_u = \langle u|A\rangle \\ \alpha_d = \langle d|A\rangle \\\] <p>It is important to remember that these components are complex number, and carry no physical meaning by themselves. However, their magnitude does. In fact, given that the spin has been prepared in state \(\vert A\rangle\), and that the apparatus is oriented along \(z\), the quantity \(\alpha_u^* \alpha_u\) is the probability that the spin would be measured as \(\sigma_z = +1\) (an <em>up</em> spin along the \(z\)-axis).</p> <p>Obviously, the same holds for \(\alpha_d^*\alpha_d\), which is the probability of measuring \(\sigma_z = -1\).</p> <p>To be more precise, the quantities \(\alpha_u\) and \(\alpha_d\) are classed probability amplitudes and are not actual probabilities. To get actual probabilities, these quantities need to be squared, so that</p> \[P_u = \langle A|u\rangle\langle u|A\rangle \\ P_d = \langle A|d\rangle\langle d|A\rangle\] <p>Something important to notice is the state \(\vert A\rangle\) is not what we measure. \(\vert A\rangle\) is something we know <em>before</em> the measurement. It is our knowledge of the state of the system, or how the system was <em>prepared</em>. It represents the <em>potential</em> possibilities of the values of our measurements. Once we measure the spin along a certain direction, however, we can only get a \(\vert u\rangle\) or a \(\vert d\rangle\).</p> <p>Two additional points are important. First, \(\vert u\rangle\) an \(\vert d\rangle\) have to be mutually orthogonal, meaning that</p> \[\langle u|d\rangle = 0 \\ \langle d|u\rangle = 0 \\\] <p>but what does this mean? It means that the <em>up</em> and <em>down</em> states are physically distinct and mutually exclusive. If the spin is prepared in the <em>up</em> state, then it can’t be detected to be in the <em>down</em> state, and viceversa. This is true not just for the spin, but for any quantum system.</p> <p>Another point is that, since <em>up</em> and <em>down</em> are the only possible result of the measurement, their respective probabilities need to sum up to 1. Mathematically,</p> \[\alpha_u^*\alpha_u + \alpha_d^*\alpha_d =1\] <p>which is the same as saying that \(\vert A\rangle\) is normalized:</p> \[\langle A| A\rangle = 1\] <p>And this is the last thing I’ll state for today, but it’s a very important principle: <em>the state of a system is represented by a unit vector in a vector space of states. Moreover, the squared magnitudes of the components of the state-vector, along particular basis vectors, represent probabilities for various experimental outcomes.</em> Prof. Susskind said that so it must be true.</p> <h3 id="what-about-next-time">What about next time?</h3> <p>To recap, we made some spin experiment, looked at the weird results we were getting, and developed a small part of a mathematical framework to represent these result, work with them, and one day make predictions.</p> <p>I think this is enough for today, as we went thought quite some stuff. In the next episode, we’ll be deriving state vectors for the spin in any direction, and state the principles of quantum mechanics!</p> <p>See ya!</p>]]></content><author><name></name></author><summary type="html"><![CDATA[A long series on learning quantum mechanics from scratch]]></summary></entry></feed>